{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zangston/CS-4774/blob/master/Homework%202/CS4774_Homework_02_final_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTZW4uoxCVeI"
      },
      "source": [
        "# Homework 02: Linear Classification\n",
        "\n",
        "Deadline: Monday March 13, 2023 11:59 PM (without a late penalty)\n",
        "\n",
        "---\n",
        "\n",
        "This homework consists two sections, which covers the Perceptron algorithm and logistic regression classifiers that we learned. \n",
        "\n",
        "If you are running this notebook on your local computer, please make sure it has the following packages installed\n",
        "\n",
        "- Numpy\n",
        "- Sklearn\n",
        "\n",
        "---\n",
        "\n",
        "**Requirements**:\n",
        "\n",
        "Please read the following requirements carefully, to avoid any unnecessary point deduction. \n",
        "\n",
        "1. Keep all the required results in your submission, our TAs will grade the homework based on the submitted results. (The TAs not run the code unless they have some questions about the implementation.)\n",
        "2. Your submission should only be via Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CizxMwf-TTtC"
      },
      "source": [
        "## 1 The Perceptron Algorithm (5 points)\n",
        "\n",
        "### 1.1 Implementation (3.5 points)\n",
        "\n",
        "Let's implement the Perceptron algorithm described in our class lecture. \n",
        "\n",
        "First, we need to download a toy dataset from the course webpage and load it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eLBWI8RrT8pN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71996a83-b245-4221-adf1-10f9ad194830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.11  0.59  1.  ]\n",
            " [ 0.35  0.79  1.  ]\n",
            " [ 0.25  0.46  1.  ]\n",
            " [ 0.03  0.26  1.  ]\n",
            " [-0.37  0.38  1.  ]\n",
            " [ 0.25  0.12  1.  ]\n",
            " [-0.06 -0.03  1.  ]\n",
            " [ 0.2  -0.2   1.  ]\n",
            " [ 0.55  0.08  1.  ]\n",
            " [ 0.81 -0.11  1.  ]\n",
            " [ 0.54 -0.23  1.  ]\n",
            " [-0.11 -0.23  1.  ]\n",
            " [ 0.14  1.03  1.  ]\n",
            " [ 0.59  1.21  1.  ]]\n",
            "[-1. -1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.]\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import numpy as np\n",
        "url = \"https://yangfengji.net/uva-ml-undergrad/data/separable.txt\"\n",
        "filename, headers = urllib.request.urlretrieve(url, filename=\"separable.txt\")\n",
        "\n",
        "data = arr = np.loadtxt(\"separable.txt\", delimiter=\"\\t\", dtype=float)\n",
        "X = data[:,:2] # Input\n",
        "Y = data[:,-1] # Label\n",
        "\n",
        "# Attach one column to X for capturing the bias term in classification\n",
        "X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
        "\n",
        "# X is the first array, with each point's coordinate\n",
        "print(X)\n",
        "# Y is the second array, which stores each point's sign\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj50cVGkWCNC"
      },
      "source": [
        "Please implement the Perceptron algorithm in the following code block.\n",
        "\n",
        "Make sure your implementation has the following components\n",
        "\n",
        "- The Percpetron updating rule and the condition of when to run the updating rule;\n",
        "- The convergence criterion -- stop training when the classifier can make correct prediction of all the training examples;\n",
        "- Print out the classification weight in the end. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Qlsq6j-KW0k2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "406cf315-69d7-4010-f665-764a744fa844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification weights: [ 2.22 -4.17  1.  ]\n"
          ]
        }
      ],
      "source": [
        "def perceptron(X, Y):\n",
        "    # initialize weight vector and bias term\n",
        "    weights = np.zeros(3)\n",
        "\n",
        "    # Set convergence criterion to false before execution\n",
        "    convergence = 0\n",
        "\n",
        "    while convergence == 0:\n",
        "        '''\n",
        "        We assume the classifier is true, and then we test its correctness\n",
        "            If assumption of correctness is true, loop ends\n",
        "            If not, we set assumption to false, which repeats the loop\n",
        "        '''\n",
        "        convergence = 1\n",
        "\n",
        "        '''\n",
        "        The notes kind of confused me with the i <- t % m line but basically \n",
        "            it looks like the intent is to be able wrap around the training set\n",
        "            to continually update the weights while the algorithm is still\n",
        "            training.\n",
        "        Using a while loop with a nested for loop implicitly uses this technique\n",
        "            without needing to declare separate t and i variables\n",
        "        '''\n",
        "        for i in range(len(X)):\n",
        "            if Y[i] * np.dot(weights, X[i]) <= 0:\n",
        "                weights += X[i] * Y[i]\n",
        "                convergence = 0\n",
        "            # print(weights)\n",
        "        # print(weights)\n",
        "\n",
        "    return weights\n",
        "\n",
        "print(\"Classification weights:\" , perceptron(X, Y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4GS1OePMtez"
      },
      "source": [
        "### 1.2 The Difference between Perceptron and Logisitic Regression (1.5 points)\n",
        "\n",
        "Please list the major difference between the Perceptron model and the Logistic Regression model as we discussed in class. Please list at least three of them.\n",
        "\n",
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EZm5mHnXs7b"
      },
      "source": [
        "1.   The perceptron model uses an iterative algorithm that updates the classification weights until the convergence criterion is satisfied. The logistic regression model uses gradient-based learning in its optimization of its predictions.\n",
        "2. The perceptron algorithm's updating rule also only applies when a prediction is wrong. This is not the same for logistic regression, whose updating rule also applies to correction predictions\n",
        "3.   The logistic regression model defines the probability of assignment of label *y*, based on the given input *x*. The perceptron model does not have the same probabilistic interpretation. \n",
        "4. Online learning vs. batch learning: the perceptron model is an instance of online learning, where the weights are updated based on the observed error for each subsequent data point. The logstic regression model uses batch learning, which is when a model makes a prediction for all data points, and updates based on the average error until convergence.\n",
        "5. Because the logistic regression uses gradient-based learning, it does not utilize a learning rate in updating its classification weights. The perceptron does, however.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_j2VR5DD6lE"
      },
      "source": [
        "## 2 Logistic Regression (9 points)\n",
        "\n",
        "Logistic regression can handle both linear separable and unseparable cases. Therefore, we will use a real-world example datasets, instead of a synthetical one. \n",
        "\n",
        "We will use the [Logistic Regression]() model from Sklearn for the questions. In other words, you don't have to implement a logistic regression model by yourself, but you need to understand how to use it and how to interpret its outputs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b8yaLJ_EpK2"
      },
      "source": [
        "### 2.0 Download the dataset\n",
        "\n",
        "Run the following code and download the [xxx]() dataset from [OpenML](). Similar to what we explained in class, we will also use the [OridinalEncoder]() to convert the non-numeric features into numeric features. \n",
        "\n",
        "Furthermore, we will run the data split function to divide the whole dataset into a training set and a validation set. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PZdc98PZndsb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hxfQSr6dFECO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f61fcc-a3c1-4fd0-e78f-15595538c987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 768 examples with 8 features\n",
            "     preg   plas  pres  skin   insu  mass   pedi   age\n",
            "0     6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0\n",
            "1     1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0\n",
            "2     8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0\n",
            "3     1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0\n",
            "4     0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0\n",
            "..    ...    ...   ...   ...    ...   ...    ...   ...\n",
            "763  10.0  101.0  76.0  48.0  180.0  32.9  0.171  63.0\n",
            "764   2.0  122.0  70.0  27.0    0.0  36.8  0.340  27.0\n",
            "765   5.0  121.0  72.0  23.0  112.0  26.2  0.245  30.0\n",
            "766   1.0  126.0  60.0   0.0    0.0  30.1  0.349  47.0\n",
            "767   1.0   93.0  70.0  31.0    0.0  30.4  0.315  23.0\n",
            "\n",
            "[768 rows x 8 columns]\n",
            "The size of training set: 537\n",
            "The size of validation set: 231\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, Y = fetch_openml(\"diabetes\", version=1, return_X_y=True)\n",
        "print(\"Read {} examples with {} features\".format(X.shape[0], X.shape[1]))\n",
        "\n",
        "print(X)\n",
        "\n",
        "trnX, valX, trnY, valY = train_test_split(X, Y, test_size=0.3, random_state=111)\n",
        "print(\"The size of training set: {}\".format(trnX.shape[0]))\n",
        "print(\"The size of validation set: {}\".format(valX.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGJ5zqnhFEiH"
      },
      "source": [
        "### 2.1 Build the Classifier (2 points)\n",
        "\n",
        "Please follow the instructions and complete the code block for building an LR classifier. \n",
        "\n",
        "- Use the [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model from Sklearn with its **default** parameters.\n",
        "- Training the model and report its prediction accuracy on **both** the training and the validation sets.\n",
        "- Keep the **printed** (instead of hand typing) results in your submission for grading. Make sure the printed results have enough information about which is training accuracy and which one is validation accuracy. For example, you can use \n",
        "```python\n",
        "print(\"Training accuracy: {}\".format(trn_acc))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fwsmDKwFFmyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e8b2375-5c16-4e3a-ab75-d62e991df42c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 0.7802607076350093\n",
            "Validation accuracy : 0.7532467532467533\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "'''\n",
        "initialize model and fit to data\n",
        "i was getting a nonconvergence error and the error message recommended i try \n",
        "    increasing the number of iterations. please god i hope it works\n",
        "IT WORKS!!!\n",
        "'''\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(trnX, trnY)\n",
        "\n",
        "# generate predictions\n",
        "predict_trnY = model.predict(trnX)\n",
        "predict_valY = model.predict(valX)\n",
        "\n",
        "# compute accuracy\n",
        "trn_acc = accuracy_score(trnY, predict_trnY)\n",
        "val_acc = accuracy_score(valY, predict_valY)\n",
        "\n",
        "# output results\n",
        "print(\"Training accuracy: {}\".format(trn_acc))\n",
        "print(\"Validation accuracy : {}\".format(val_acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQCfRmwmFnqP"
      },
      "source": [
        "### 2.2 Feature Pre-processing (2 points)\n",
        "\n",
        "In our lecture, we talk about three different ways to represent the features before feeding into the classifier. \n",
        "\n",
        "Pick the [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) to further processing the features, and building the classifier again. \n",
        "\n",
        "Please follow the instructions and complete the code block for building an LR classifier with feature pre-processing\n",
        "\n",
        "- Pick one pre-processing method to further scale or encode the features\n",
        "- Training the model and report its prediction accuracy on **both** the training and the validation sets.\n",
        "- Keep the **printed** (instead of hand typing) results in your submission for grading. Make sure the printed results have enough information about which is training accuracy and which one is validation accuracy.\n",
        "\n",
        "Note that, you can find many other pre-processing functions in Sklearn via [this link](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing). Since we only discussed the three abovementioned pre-processing methods in class, you are *not* required to do anything beyond the instruction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "aWDbmNt5IiXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81264169-4323-46d0-d046-6da894c2b436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  0.9739292364990689\n",
            "Validation Accuracy:  0.7142857142857143\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# initialize encoder, train on both sets\n",
        "oneHot = OneHotEncoder(handle_unknown='ignore')\n",
        "oneHot.fit(trnX)\n",
        "trnOneHot = oneHot.transform(trnX)\n",
        "valOneHot = oneHot.transform(valX)\n",
        "\n",
        "# fit logistic regression to data\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(trnOneHot, trnY)\n",
        "\n",
        "# generate predictions\n",
        "predict_trnY = model.predict(trnOneHot)\n",
        "predict_valY = model.predict(valOneHot)\n",
        "\n",
        "# calculate accuracies\n",
        "trn_acc = accuracy_score(trnY, predict_trnY)\n",
        "val_acc = accuracy_score(valY, predict_valY)\n",
        "\n",
        "# print results\n",
        "print(\"Training Accuracy: \", trn_acc)\n",
        "print(\"Validation Accuracy: \", val_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3nrHVI2IjPB"
      },
      "source": [
        "### 2.3 Justification of Pre-processing (2 points)\n",
        "\n",
        "If your implementation is correct, you should see a significant boost on both training and validation accuracies. \n",
        "\n",
        "Based on what we discussed in class, please explain why this one-hot encoding is helpful for improving classification performance.\n",
        "\n",
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JySvBz_4Y-s-"
      },
      "source": [
        "One-hot encoding allows categorical variables to be split into their own boolean vector \"dummy\" variables, for each category. This formats categorical data that is more easily interpreted by the learning algorithm and allows the  model to better understand the effect of any particular category being \"true\", all else equal.\n",
        "\n",
        "This technique is also applied when creating regression models in statistical and econometric contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8Zlz48wJyOS"
      },
      "source": [
        "### 2.4 $\\ell_2$ Regularization (1.5 points)\n",
        "\n",
        "With the pre-processed data, please train the classifiers with three different regularization coefficient. In the [Logistic Regression]() model implemented in Sklearn, the associated function argument is $C$. \n",
        "\n",
        "Please use the following three code blocks and try three different $C$ values\n",
        "\n",
        "- $C=100.0$\n",
        "- $C=1.0$\n",
        "- $C=0.01$\n",
        "\n",
        "and pirnt out \n",
        "\n",
        "- the training accuracy, and \n",
        "- the validation accuracy \n",
        "\n",
        "by following the **same** format as the previous questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnvXilP4K3W4"
      },
      "source": [
        "$C=100.0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "oap5h_QeKFej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c17d63-886b-4245-d683-7336e5c496d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  1.0\n",
            "Validation Accuracy:  0.7229437229437229\n"
          ]
        }
      ],
      "source": [
        "# fit logistic regression to data\n",
        "model = LogisticRegression(max_iter=1000, C = 100.0)\n",
        "model.fit(trnOneHot, trnY)\n",
        "\n",
        "# generate predictions\n",
        "predict_trnY = model.predict(trnOneHot)\n",
        "predict_valY = model.predict(valOneHot)\n",
        "\n",
        "# calculate accuracies\n",
        "trn_acc = accuracy_score(trnY, predict_trnY)\n",
        "val_acc = accuracy_score(valY, predict_valY)\n",
        "\n",
        "# print results\n",
        "print(\"Training Accuracy: \", trn_acc)\n",
        "print(\"Validation Accuracy: \", val_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZboa3xPK6vI"
      },
      "source": [
        "$C=1.0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xwAjR11sK7-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b78661e-113f-4fca-f957-9ca3302159e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  0.9739292364990689\n",
            "Validation Accuracy:  0.7142857142857143\n"
          ]
        }
      ],
      "source": [
        "# fit logistic regression to data\n",
        "model = LogisticRegression(max_iter=1000, C = 1.0)\n",
        "model.fit(trnOneHot, trnY)\n",
        "\n",
        "# generate predictions\n",
        "predict_trnY = model.predict(trnOneHot)\n",
        "predict_valY = model.predict(valOneHot)\n",
        "\n",
        "# calculate accuracies\n",
        "trn_acc = accuracy_score(trnY, predict_trnY)\n",
        "val_acc = accuracy_score(valY, predict_valY)\n",
        "\n",
        "# print results\n",
        "print(\"Training Accuracy: \", trn_acc)\n",
        "print(\"Validation Accuracy: \", val_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TndTQrHKLHFw"
      },
      "source": [
        "$C=0.01$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Z9smqQS8LGZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d15e42be-923d-45a1-f0d8-339492830d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  0.6443202979515829\n",
            "Validation Accuracy:  0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "# fit logistic regression to data\n",
        "model = LogisticRegression(max_iter=1000, C = 0.01)\n",
        "model.fit(trnOneHot, trnY)\n",
        "\n",
        "# generate predictions\n",
        "predict_trnY = model.predict(trnOneHot)\n",
        "predict_valY = model.predict(valOneHot)\n",
        "\n",
        "# calculate accuracies\n",
        "trn_acc = accuracy_score(trnY, predict_trnY)\n",
        "val_acc = accuracy_score(valY, predict_valY)\n",
        "\n",
        "# print results\n",
        "print(\"Training Accuracy: \", trn_acc)\n",
        "print(\"Validation Accuracy: \", val_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjrDQry6LRFp"
      },
      "source": [
        "### 2.5 Explan the regularization effect (1.5 points)\n",
        "\n",
        "Based on the classification accuracies above, explain the $\\ell_2$ regularization effect for the classification performance\n",
        "\n",
        "- What is the direct effect of $\\ell_2$ regularization?\n",
        "- Does it work all the time or when it may work?\n",
        "- How do we know it works for helping avoid over-fitting?\n",
        "\n",
        "Make sure your answer covers all the three questions above.\n",
        "\n",
        "**Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNXQ5LZCNCJ_"
      },
      "source": [
        "ℓ2 regularization changes the loss function of the logistic regression model in a way that penalizes higher complexity in a model. This encourages the learning model to estimate a simpler fit.\n",
        "\n",
        "ℓ2 regularization works in situations where a dataset has many features, some of which might be irrelevant. The modified loss function encourages the learning model to attempt to \"drop\" certain categorical variables, driving their weights to zero. Inversely, ℓ2 will hurt the accuracy of a model if there are many equally important features, as it penalizes higher weights. If all weights are equally important, the model will naturally be complex, which is what ℓ2 sets out to prevent.\n",
        "\n",
        "The primary goal of ℓ2 regularization is to punish higher weight assignments. Overfitting occurs when a model is too complex, and the regression is too closely fitted to a dataset, losing efficacy in light of newly introduced data. Thus regularization, encouraging a simpler model via smaller weights, preserves a model's general goodness-of-fit, which prevents overfitting."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
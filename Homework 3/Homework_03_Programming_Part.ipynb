{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 03: Bias-Complexity Tradeoff\n",
        "\n",
        "**Note**\n",
        "\n",
        "- Please keep the results in the iPython notebook of your submission for grading\n",
        "- Please make sure you also finish the written part"
      ],
      "metadata": {
        "id": "wREovaIC5yRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Data Generation\n",
        "\n",
        "In the following code, we use the [make_circles]() function from Sklearn to create a 2-D dataset, which in total has 2K samples. \n",
        "\n",
        "Similar to what we did in the regression demo, we will use 50% of them to create a test set. In practice, we prefer to use a relatively smaller proption of our data to construct the test set. However, in this case, we want to make the test set is a good approximation of the underlying data distribution. \n",
        "\n",
        "**Note**: the role of the test set in this project is similar to the validation set in the model selection lecture, to test the performance of each selected hyper-parameter. \n",
        "\n",
        "In the end, the code will create three sets of data \n",
        "\n",
        "- (Xtrn_large, Ytrn_large): a large training set with 1000 examples. \n",
        "- (Xtrn, Ytrn): a small subset of (Xtrn_large, Ytrn_large), containing 20 examples, which is also the one dataset we will use to mimic the training set in practice. If you are unclear about the difference between this one and the previous one, please review our demo code on the Bias-Complexity Tradeoff first.\n",
        "- (Xtst, Ytst): the test set with another 1000 examples."
      ],
      "metadata": {
        "id": "lOxSYcHa56ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please keep this code block unchanged\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_gaussian_quantiles, make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "s = 2\n",
        "np.random.seed(s)\n",
        "random.seed(s)\n",
        "\n",
        "\n",
        "X, Y = make_circles(n_samples=2000, noise=0.15, random_state=1, factor=0.7)\n",
        "Xtrn_large, Xtst, Ytrn_large, Ytst = train_test_split(X, Y, test_size=0.5)\n",
        "print(Xtrn_large.shape)\n",
        "Xtrn, _, Ytrn, _ = train_test_split(Xtrn_large, Ytrn_large, test_size=0.98)\n",
        "print(Xtrn.shape)"
      ],
      "metadata": {
        "id": "0zM5w3i754Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the training set (Xtrn, Ytrn).\n",
        "\n",
        "By replacing it with (Xtrn_large, Ytrn_large), we can also get a glimpse of the underlying data distribution. "
      ],
      "metadata": {
        "id": "Rhdwjmr9CEO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "color_list = []\n",
        "for idx in list(Ytrn):\n",
        "    if idx == 0:\n",
        "        color_list.append('blue')\n",
        "    else:\n",
        "        color_list.append('red')\n",
        "\n",
        "plt.scatter(Xtrn[:,0], Xtrn[:,1], color=color_list)"
      ],
      "metadata": {
        "id": "_jVblX3v7I-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Build the Classifier $h(x)$ (3 points)\n",
        "\n",
        "In the following code block, please implement a SVM classifier with the RBF kernel. \n",
        "\n",
        "\n",
        "The classifier that we are going to build is the classifier trained on (Xtrn, Ytrn). \n",
        "\n",
        "\n",
        "Note that, your implementation should use the exact same arguments as listed in the following `classifier` function. \n",
        "\n",
        "You can use the `SVC` function defined in the [sklearn]() package for your implementation, but please keep the default parameters, except the following two\n",
        "\n",
        "- `kernel`\n",
        "- `gamma`\n",
        "\n",
        "With your implementation, please report the prediction accuracy on the test set with the following hyper-parameter values\n",
        "\n",
        "- `kernel` as `rbf`\n",
        "- `gamma` as 1\n",
        "\n",
        "Please print out the accuracy number on the validation set and leave it there for grading. "
      ],
      "metadata": {
        "id": "2zAyXJmg8pxw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYLtm1OK8wOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Build the Optimal Classifier $h^{\\ast}$ (2 points)\n",
        "\n",
        "The second classifier that we need is the optimal classifier for a pre-defined hypothesis class. \n",
        "\n",
        "In this project, a SVM with the RBF kernal and a specific `gamma` can be considered as one pre-defined hypothesis class. Then, the task is to find the optimal classifier in this class. \n",
        "\n",
        "Following the strategy used in the class demo, please implement the code of finding the optimal classifier in the following block. \n",
        "\n",
        "You can use the code from the previous block as much as you can, and report the validation accuracy with the following hypoer-parameter values\n",
        "\n",
        "- `kernel` as `rbf`\n",
        "- `gamma` as 1\n",
        "\n",
        "Please keep the printed accuracy number for grading purpose."
      ],
      "metadata": {
        "id": "DWGMweAj7P-v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "biNQkE5U0cO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Build the oracle classifier $f$\n",
        "\n",
        "According to the [make_circles](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html) function, the ground truth decision boundary is the radius = 0.8, which means we can compute the distance between a data point and the origin for predicting its label. \n",
        "\n",
        "Please take a look the implementation and make sure you understand how to use it. "
      ],
      "metadata": {
        "id": "PY-EXjOfe5_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please keep this code block unchanged\n",
        "\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def f(X, Y, radius):\n",
        "    d = norm(X, axis=1)\n",
        "    pred_labels = np.zeros(d.shape)\n",
        "    pred_labels[np.less_equal(d, radius)] = 1\n",
        "    acc = (pred_labels == Y).sum() / (1.0*Y.shape[0])\n",
        "    return acc\n",
        "\n",
        "\n",
        "def learn_f(Xtst, Ytst, iterations=200):\n",
        "    ## Use linear search to find the best radius for the classifier $f$\n",
        "    r_lower, r_upper = 0.7, 1.2\n",
        "    r_current = (r_lower + r_upper)/2\n",
        "    thresholds = np.linspace(r_lower, r_upper, iterations)\n",
        "    accs = []\n",
        "    for val in thresholds:\n",
        "        acc = f(Xtst, Ytst, val)\n",
        "        accs.append(acc)\n",
        "    idx = np.argmax(accs)\n",
        "    return accs[idx]\n",
        "\n",
        "\n",
        "acc_f = learn_f(Xtst, Ytst, iterations=500)\n",
        "print(\"The oracle classifier accuracy: {}\".format(acc_f))"
      ],
      "metadata": {
        "id": "H62ktjiv3ldb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Bias-Complexity Tradeoff (4 points)\n",
        "\n",
        "By choosing different degrees of polynomial kernel, essentially we are choosing the hypothesis classes with different complexities. Intuitively, a higher degree gives the hypothesis class with higher complexity. \n",
        "\n",
        "In our lecture, we discussed the fundamental tradeoff between the bias and the complexity for a given hypothesis class. \n",
        "\n",
        "Similar to the regression case shown in our class demo, the bias term can be measured by the following formula\n",
        "\n",
        "$$\\text{acc}(f) - \\text{acc}(h^{\\ast})$$\n",
        "\n",
        "which measures the difference between the optimal classifier from a given hypothesis class (section 2) and the oracle classifier (section 3). \n",
        "\n",
        "The complexity term can be measured as the difference between the optimal classifier $h^{\\ast}$ and the learned classifier $f$ with limited training examples\n",
        "\n",
        "$$\\text{acc}(h^{\\ast}) - \\text{acc}(h)$$\n",
        "\n",
        "In the following code block, please implement these two formula, and report the accuracy differences with the pre-defined gamma's"
      ],
      "metadata": {
        "id": "VIdALvkN969z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m0KTsQraBrPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make sure you also submit the written part**\n",
        "\n",
        "Please checkout the pdf file for the questions. \n",
        "\n",
        "\n",
        "**THE END**\n",
        "\n",
        "--- \n",
        "\n"
      ],
      "metadata": {
        "id": "wVBcGM2EB3YE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9xmNWAfXcfDm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}